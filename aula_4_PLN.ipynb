{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Normalização de Texto e Remoção de Ruído"
      ],
      "metadata": {
        "id": "EILTddKFHr-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    2. Normalização de Texto e Remoção de Ruído\n",
        "Esta etapa remove caracteres especiais, pontuações e padroniza o uso de letras maiúsculas e minúsculas."
      ],
      "metadata": {
        "id": "pSQmgkZ7GvS8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B1E1BF_CHhU",
        "outputId": "7a997c82-ec50-41d4-922f-1c3606a8c3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá!!! Estou tão feliz e agradecida pelo aprendizado de hoje. É uma preparação para o mercado de trabalho\n",
            "\n",
            "texto limpo: Ol!!! Estou to feliz e agradecida pelo aprendizado de hoje.  uma preparao para o mercado de trabalho\n",
            "\n",
            "texto normalizado: ol!!! estou to feliz e agradecida pelo aprendizado de hoje.  uma preparao para o mercado de trabalho\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "original = \"Olá!!! Estou tão feliz e agradecida pelo aprendizado de hoje. É uma preparação para o mercado de trabalho\"\n",
        "texto_limpo = re.sub(r'[^A - Z, a - z, À -y\\s]','', original)\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\ntexto limpo: {texto_limpo}')\n",
        "print(f'\\ntexto normalizado: {texto_normalizado}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenização divide o texto em unidades menores (tokens), que geralmente são palavras."
      ],
      "metadata": {
        "id": "ZAk5YYW4KzaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "tokens = word_tokenize(texto_normalizado)\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\n\\ntexto limpo: {texto_limpo}')\n",
        "print(f'\\n\\ntexto normalizado: {texto_normalizado}')\n",
        "print(f'\\n\\nTokens extraidos: {tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa1YC7DziBXy",
        "outputId": "846f0849-eb6e-4621-8e6f-8af411763189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá!!! Estou tão feliz e agradecida pelo aprendizado de hoje. É uma preparação para o mercado de trabalho\n",
            "\n",
            "\n",
            "texto limpo: Ol!!! Estou to feliz e agradecida pelo aprendizado de hoje.  uma preparao para o mercado de trabalho\n",
            "\n",
            "\n",
            "texto normalizado: ol!!! estou to feliz e agradecida pelo aprendizado de hoje.  uma preparao para o mercado de trabalho\n",
            "\n",
            "\n",
            "Tokens extraidos: ['ol', '!', '!', '!', 'estou', 'to', 'feliz', 'e', 'agradecida', 'pelo', 'aprendizado', 'de', 'hoje', '.', 'uma', 'preparao', 'para', 'o', 'mercado', 'de', 'trabalho']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "print(stopwords_pt)\n",
        "tokens_sem_stopwords =[palavra for palavra in tokens if palavra.lower()not in stopwords_pt]\n",
        "\n",
        "print(f'\\n\\nTokens extraidos: {tokens} + \\n quantidade de tokens: {len(tokens)}')\n",
        "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords} + \\n quantidade de tokens: {len(tokens_sem_stopwords)}\\n')"
      ],
      "metadata": {
        "id": "U7XiD_BtiUkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df019d3-0ce6-4ed5-c0ed-58624a72a877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hei', 'sejam', 'de', 'tenham', 'isso', 'seus', 'houve', 'estivermos', 'vocês', 'teria', 'houvermos', 'serei', 'forem', 'na', 'eles', 'terá', 'tiver', 'serão', 'vos', 'estiveram', 'dele', 'estejam', 'tenho', 'estas', 'tuas', 'como', 'havemos', 'houvessem', 'um', 'das', 'às', 'lhe', 'houveremos', 'for', 'sou', 'tinham', 'tivéramos', 'tivesse', 'estes', 'estejamos', 'teriam', 'esse', 'formos', 'houverá', 'era', 'estive', 'a', 'aqueles', 'ser', 'entre', 'esta', 'minhas', 'estivera', 'houvera', 'seremos', 'houverão', 'que', 'estivessem', 'tivera', 'esses', 'depois', 'sem', 'sejamos', 'suas', 'sua', 'pelos', 'é', 'houver', 'você', 'hajamos', 'aquele', 'estar', 'dos', 'tínhamos', 'dela', 'está', 'fôssemos', 'por', 'tivemos', 'aquilo', 'em', 'houverem', 'nosso', 'este', 'lhes', 'estava', 'muito', 'nossa', 'seria', 'tiveram', 'até', 'nós', 'isto', 'tua', 'houveram', 'tivéssemos', 'qual', 'estivéramos', 'nem', 'houverei', 'teve', 'com', 'eram', 'hajam', 'tinha', 'estão', 'pela', 'tenha', 'aquelas', 'elas', 'foi', 'fomos', 'fossem', 'nossas', 'pelas', 'deles', 'esteja', 'terei', 'houvéramos', 'tiverem', 'fosse', 'tivermos', 'estávamos', 'são', 'teríamos', 'tivessem', 'essa', 'estiver', 'numa', 'me', 'minha', 'estiverem', 'o', 'meu', 'seja', 'num', 'da', 'e', 'éramos', 'será', 'fui', 'foram', 'hão', 'houvemos', 'ele', 'à', 'tu', 'tem', 'já', 'estivéssemos', 'somos', 'quando', 'ao', 'houveriam', 'mais', 'teu', 'seriam', 'do', 'eu', 'esteve', 'teus', 'os', 'delas', 'mas', 'nas', 'houvesse', 'também', 'se', 'não', 'houveríamos', 'temos', 'seríamos', 'houvéssemos', 'haja', 'aquela', 'para', 'tenhamos', 'estou', 'ou', 'só', 'te', 'terão', 'uma', 'tive', 'mesmo', 'fora', 'tém', 'houveria', 'estamos', 'aos', 'essas', 'pelo', 'estavam', 'estivesse', 'fôramos', 'há', 'nossos', 'seu', 'ela', 'estivemos', 'teremos', 'nos', 'quem', 'as', 'no', 'meus', 'haver'}\n",
            "\n",
            "\n",
            "Tokens extraidos: ['ol', '!', '!', '!', 'estou', 'to', 'feliz', 'e', 'agradecida', 'pelo', 'aprendizado', 'de', 'hoje', '.', 'uma', 'preparao', 'para', 'o', 'mercado', 'de', 'trabalho'] + \n",
            " quantidade de tokens: 21\n",
            "\n",
            "\n",
            "Tokens extraidos: ['ol', '!', '!', '!', 'to', 'feliz', 'agradecida', 'aprendizado', 'hoje', '.', 'preparao', 'mercado', 'trabalho'] + \n",
            " quantidade de tokens: 13\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    4. Remoção de Stopwords\n",
        "Stopwords são palavras de pouco valor semântico (como \"de\", \"a\", \"o\") que podem ser removidas para focar nas palavras mais relevantes."
      ],
      "metadata": {
        "id": "uOnN-ZtVilp4"
      }
    }
  ]
}